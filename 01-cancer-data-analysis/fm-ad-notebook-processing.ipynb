{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Up Research Software Development with Github Copilot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb), we conducted various data exploration techniques to gain a deeper understanding of the dataset. Now equipped to make informed decisions regarding data cleaning. Let's load the CSV file generated from the [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb) and reference it as a dataframe so that we can start working with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the combined_data.csv to dataframe called df\n",
    "df = pd.read_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Set output display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively view and analyze the dataset, we need to configure pandas to display all columns and most rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)  # or 1000\n",
    "pd.set_option(\"display.max_rows\", None)  # or 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dataset documentation, there are 18,004 records in the study. Additionally, results from section 2.6 of [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb) indicate that the `case_id` column has 18,004 unique values.\n",
    "\n",
    "Moreover, other columns also have 18,004 unique values. These columns likely serve as unique identifiers similar to the `case_id` column. For the purposes of this workshop, we can assume they are redundant.\n",
    "\n",
    "Let's create a prompt to identify which of these columns fit this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary called col_remove to store names of columns that have great than or equal to 18004 unique values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the list above, the `case_id` column is included in the list. However, since we want to use this list to specifiy which columns to remove from the dataframe, we should remove `case_id` from this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove case_id from the dictionary above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the `case_id` column has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show if case_id is in the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from the dictionary above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns in the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Normalizing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we handle duplicate records, let's normalize the notation for missing values first. Currently, missing values are listed as 'Unkown' or NaN. Let's convert them all to NaN for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values 'Unknown' to NaN in the dataframe using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should verify if the operation above was done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any columns still have the value 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Handling duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in section 2.7 of [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb) notebook that there were duplicate records. Let's go ahead and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate records in the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dimensions of the new dataframe. If the duplicates were dropped succesfully, there should be half the amount of records in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2.7, you saw that there were records that shared the same `case_id`. Let's check if there are any other records share a `case_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records share the same case_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a visual representation of the distribution of the number of records that are shared by `case_id`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a bar graph with the x axis as the number of records shared by case_id and the y axis as the number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the list of records shared by a particular `case_id`.\n",
    "\n",
    "Create a prompt below to generate code to show you records that shares a case_id different from the case_id in section 2.7 of [fm-ad-notebook-exploration.ipynb](fm-ad-notebook-exploration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the records with the case_id fcd9637f-00f2-49e9-bb87-94e556d5d7eb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created prompts that are called zero-shot prompts. Basically it means that these prompts have no specific examples, we just tell it to do what we want.\n",
    "\n",
    "Next we will be working with one-shot prompts. In addition to describing what you want like in zero-shot prompts, one-shot prompts are prompts adds the prompt with a single example. This helps generate a more context-aware response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the two records with the same 'case_id' value of 'fcd9637f-00f2-49e9-bb87-94e556d5d7eb.\n",
    "# Display these records for visual inspection. Then, verify that these records complement each other in terms of null and non-null values for all columns after the first five columns.\n",
    "# In other words, if one record has NaN values in a column, the other record should have non-NaN values in that same column, and vice versa.\n",
    "# Also, if both records have NaN values in the same column and ignore it from the comparison\n",
    "# If the two records complement each other, print \"The two records complement each other.\" Otherwise, print \"The two records do not complement each other.\"\n",
    "\n",
    "# get the records with the case_id fcd9637f-00f2-49e9-bb87-94e556d5d7eb\n",
    "case_id_records = df[df['case_id'] == 'fcd9637f-00f2-49e9-bb87-94e556d5d7eb']\n",
    "\n",
    "# get the first record\n",
    "record1 = case_id_records.iloc[0]\n",
    "\n",
    "# get the second record\n",
    "record2 = case_id_records.iloc[1]\n",
    "\n",
    "# compare the two records\n",
    "complement = True\n",
    "for column in record1.index[5:]:\n",
    "    if record1[column] != record2[column]:\n",
    "        if pd.isnull(record1[column]) or pd.isnull(record2[column]):\n",
    "            continue\n",
    "        else:\n",
    "            complement = False\n",
    "            break\n",
    "\n",
    "if complement:\n",
    "    print(\"The two records complement each other.\")\n",
    "else:\n",
    "    print(\"The two records do not complement each other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, it looks like these records compliment each other. We need to verify that all records with the same `case_id` perfectly complement each other in terms of null and non-null values. If one record has NaN values in a column, the other records should have non-NaN values in that same column, and vice versa. This step ensures data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a Python code snippet that iterates over all unique 'case_id' values. For each 'case_id', select all records associated with that 'case_id'.\n",
    "# Verify that these records complement each other in terms of null and non-null values for all columns AFTER the first five columns.\n",
    "# In other words, if one record has NaN values in a column, the other records should have non-NaN values in that same column, and vice versa.\n",
    "# Print a dictionary where each 'case_id' is a key and the corresponding value is a boolean indicating whether all records with that 'case_id' perfectly complement each other in terms of null and non-null values.\n",
    "# For example, if all records with 'case_id' = 'fcd9637f-00f2-49e9-bb87-94e556d5d7eb' perfectly complement each other, the dictionary should have the key 'fcd9637f-00f2-49e9-bb87-94e556d5d7eb' with a value of True.\n",
    "\n",
    "# create a dictionary to store the case_id and the boolean value\n",
    "case_id_dict = {}\n",
    "\n",
    "# iterate over all unique case_id values\n",
    "for case_id in df['case_id'].unique():\n",
    "    # get the records with the case_id\n",
    "    case_id_records = df[df['case_id'] == case_id]\n",
    "    complement = True\n",
    "    for i in range(len(case_id_records)):\n",
    "        record1 = case_id_records.iloc[i]\n",
    "        for j in range(i+1, len(case_id_records)):\n",
    "            record2 = case_id_records.iloc[j]\n",
    "            for column in record1.index[5:]:\n",
    "                if record1[column] != record2[column]:\n",
    "                    if pd.isnull(record1[column]) or pd.isnull(record2[column]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        complement = False\n",
    "                        break\n",
    "            if not complement:\n",
    "                break\n",
    "        if not complement:\n",
    "            break\n",
    "    case_id_dict[case_id] = complement\n",
    "\n",
    "case_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the values in the dictionary are True if so print \"All records complement each other.\" otherwise print \"Not all records complement each other.\"\n",
    "if all(case_id_dict.values()):\n",
    "    print(\"All records complement each other.\")\n",
    "else:\n",
    "    print(\"Not all records complement each other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine records with the same `case_id` by taking the first non-null value for each group. This step consolidates the data into a more concise format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine records with the same 'case_id' and take the first non-null value for each group\n",
    "df = df.groupby('case_id').first().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape and the first few records of our dataframe to ensure the data consolidation was successful. A success should show that the dataframe has 18,004 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure there are no duplicate records in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of duplicate records in the dataframe\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the number of unique values in each column to identify any potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of unique values in each column in descending order\n",
    "df.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are still any empty values in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are any null values in the dataframe\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of unique values in the columns that have null values to understand the extent of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number unique values of the columns that have null values\n",
    "df.isnull().sum()[df.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Normalizing age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by normalizing the diagnoses.age_at_diagnosis column which represents age of participants. As you will see, the diagnoses.age_at_diagnosis column does not represent age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe stats on diagnoses.age_at_diagnosis column\n",
    "df['diagnoses.age_at_diagnosis'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the publication associated with this dataset, the youngest age of the participant is 19. Let's do some basic math to calulate our normalization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6947/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the calculation above, the column's normalization factor is 365. So let's transform the existing age column by dividing it by 365 and create a new column and dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'diagnoses.age_at_diagnosis_years' by dividing 'diagnoses.age_at_diagnosis' by 365, and drop the 'diagonses.age_at_diagnosis' column\n",
    "df['diagnoses.age_at_diagnosis_years'] = df['diagnoses.age_at_diagnosis'] / 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The publication “High-Throughput Genomic Profiling of Adult Solid Tumors Reveals Novel Insights into Cancer Pathogenesis”, http://cancerres.aacrjournals.org/content/77/9/2464.long, removes records of patients aged 89 or older. Let's do some data cleaning to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many records that have the value of 'diagnosis.age_at_diagnosis_years' greater or equal to 89\n",
    "(df['diagnoses.age_at_diagnosis_years'] >= 89).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the record with 'diagnosis.age_at_diagnosis_years' greater or equal to 89\n",
    "df = df[df['diagnoses.age_at_diagnosis_years'] < 89]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the age column stores the ages of the participants as floats. The publication however describes and visualizes the result of the age distribution as integers. Given that information, let's do more data transformation to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round down the diagnoses.age_at_diagnosis_years column and convert to integer\n",
    "df['diagnoses.age_at_diagnosis_years'] = df['diagnoses.age_at_diagnosis_years'].apply(np.floor).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the range of the new age column is as expected. The proper range should be between 19-88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show statistical summary of the diagnoses.age_at_diagnosis_years column\n",
    "df['diagnoses.age_at_diagnosis_years'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the range is expected, we can drop the diagnosis.age_at_diagnosis column so that in the next notebook, GitHub Copilot is able to automatically choose our single \"age column\" for vidsualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop diagnosis.age_at_diagnosis column\n",
    "df.drop(columns=['diagnoses.age_at_diagnosis'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Save cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned dataframe to a new CSV file named combined_data_cleaned.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to a new csv file named combined_data_cleaned.csv\n",
    "df.to_csv('combined_data_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
